{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 20\n",
    "\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_portion = .8\n",
    "\n",
    "#Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\n",
    "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\",\n",
    "             \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \n",
    "             \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \n",
    "             \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\",\n",
    "             \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \n",
    "             \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \n",
    "             \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \n",
    "             \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\",\n",
    "             \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\",\n",
    "             \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \n",
    "             \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\",\n",
    "             \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\n",
    "             \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\",\n",
    "             \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \n",
    "             \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \n",
    "             \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \n",
    "             \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \n",
    "             \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \n",
    "             \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \n",
    "             \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\",\n",
    "             \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\",\n",
    "             \"yours\", \"yourself\", \"yourselves\" ]\n",
    "\n",
    "sentences = []; labels = []\n",
    "\n",
    "with open('D:/44754/Documents/Data/bbc-text2.csv','r') as csvfile:\n",
    "    reader = csv.reader(csvfile,delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        labels.append(row[0])\n",
    "        sentence = row[1]\n",
    "        for word in stopwords:\n",
    "            token = \" \"+word+\" \"\n",
    "            sentence = sentence.replace(token,\" \")\n",
    "        sentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(sentences) * training_portion)\n",
    "train_sentences = sentences[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "validation_sentences = sentences[train_size:]\n",
    "validation_labels = labels[train_size:]\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n",
    "validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
    "validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)\n",
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(labels)\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
    "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.fc1 = nn.Linear(16, 24)\n",
    "        self.fc2 = nn.Linear(24, 6)\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x,1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 120)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.asarray(train_padded[:3],dtype=np.int64)\n",
    "tmp = torch.from_numpy(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44754\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "a = network(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "    transform=torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "    batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-93-3614bc343db9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \"\"\"\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[1;31m# put it from HWC to CHW format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for data,target in train_loader:\n",
    "    print(target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for i in range(89):\n",
    "    optimizer.zero_grad()\n",
    "    tmpdata = torch.from_numpy(np.asarray(\n",
    "        train_padded[i*20:(i+1)*20],dtype=np.int64))\n",
    "    output = network(tmpdata)\n",
    "    tmptarget = torch.from_numpy(np.asarray(\n",
    "        training_label_seq[i*20:(i+1)*20].squeeze(),dtype=np.int64))\n",
    "    loss = F.nll_loss(output, tmptarget)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, i * 20, 1780,\n",
    "        100. *i / 89, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for i in range(89):\n",
    "        tmpdata = torch.from_numpy(np.asarray(\n",
    "            train_padded[i*5:(i+1)*5],dtype=np.int64))\n",
    "        tmptarget = torch.from_numpy(np.asarray(\n",
    "            training_label_seq[i*5:(i+1)*5].squeeze(),dtype=np.int64))\n",
    "        output = network(tmpdata)\n",
    "        test_loss += F.nll_loss(output,tmptarget, size_average=False).item()\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(tmptarget.data.view_as(pred)).sum()\n",
    "    test_loss /= 89\n",
    "    test_losses.append(test_loss)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, 445,\n",
    "        100. * correct / 445))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44754\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 7.2715, Accuracy: 189/445 (42%)\n",
      "\n",
      "Train Epoch: 1 [0/1780 (0%)]\tLoss: 1.361921\n",
      "Train Epoch: 1 [400/1780 (100%)]\tLoss: 1.434720\n",
      "Train Epoch: 1 [800/1780 (200%)]\tLoss: 1.446378\n",
      "Train Epoch: 1 [1200/1780 (300%)]\tLoss: 1.565656\n",
      "Train Epoch: 1 [1600/1780 (400%)]\tLoss: 1.509291\n",
      "\n",
      "Test set: Avg. loss: 7.2676, Accuracy: 180/445 (40%)\n",
      "\n",
      "Train Epoch: 2 [0/1780 (0%)]\tLoss: 1.355022\n",
      "Train Epoch: 2 [400/1780 (100%)]\tLoss: 1.428856\n",
      "Train Epoch: 2 [800/1780 (200%)]\tLoss: 1.442015\n",
      "Train Epoch: 2 [1200/1780 (300%)]\tLoss: 1.564683\n",
      "Train Epoch: 2 [1600/1780 (400%)]\tLoss: 1.504708\n",
      "\n",
      "Test set: Avg. loss: 7.2432, Accuracy: 178/445 (40%)\n",
      "\n",
      "Train Epoch: 3 [0/1780 (0%)]\tLoss: 1.348040\n",
      "Train Epoch: 3 [400/1780 (100%)]\tLoss: 1.422331\n",
      "Train Epoch: 3 [800/1780 (200%)]\tLoss: 1.436859\n",
      "Train Epoch: 3 [1200/1780 (300%)]\tLoss: 1.563712\n",
      "Train Epoch: 3 [1600/1780 (400%)]\tLoss: 1.500099\n",
      "\n",
      "Test set: Avg. loss: 7.2186, Accuracy: 181/445 (41%)\n",
      "\n",
      "Train Epoch: 4 [0/1780 (0%)]\tLoss: 1.340868\n",
      "Train Epoch: 4 [400/1780 (100%)]\tLoss: 1.415647\n",
      "Train Epoch: 4 [800/1780 (200%)]\tLoss: 1.431545\n",
      "Train Epoch: 4 [1200/1780 (300%)]\tLoss: 1.562691\n",
      "Train Epoch: 4 [1600/1780 (400%)]\tLoss: 1.495576\n",
      "\n",
      "Test set: Avg. loss: 7.1937, Accuracy: 181/445 (41%)\n",
      "\n",
      "Train Epoch: 5 [0/1780 (0%)]\tLoss: 1.333693\n",
      "Train Epoch: 5 [400/1780 (100%)]\tLoss: 1.408817\n",
      "Train Epoch: 5 [800/1780 (200%)]\tLoss: 1.426013\n",
      "Train Epoch: 5 [1200/1780 (300%)]\tLoss: 1.561763\n",
      "Train Epoch: 5 [1600/1780 (400%)]\tLoss: 1.490969\n",
      "\n",
      "Test set: Avg. loss: 7.1685, Accuracy: 185/445 (42%)\n",
      "\n",
      "Train Epoch: 6 [0/1780 (0%)]\tLoss: 1.326576\n",
      "Train Epoch: 6 [400/1780 (100%)]\tLoss: 1.401870\n",
      "Train Epoch: 6 [800/1780 (200%)]\tLoss: 1.420406\n",
      "Train Epoch: 6 [1200/1780 (300%)]\tLoss: 1.560960\n",
      "Train Epoch: 6 [1600/1780 (400%)]\tLoss: 1.486328\n",
      "\n",
      "Test set: Avg. loss: 7.1432, Accuracy: 183/445 (41%)\n",
      "\n",
      "Train Epoch: 7 [0/1780 (0%)]\tLoss: 1.319494\n",
      "Train Epoch: 7 [400/1780 (100%)]\tLoss: 1.394809\n",
      "Train Epoch: 7 [800/1780 (200%)]\tLoss: 1.414609\n",
      "Train Epoch: 7 [1200/1780 (300%)]\tLoss: 1.560179\n",
      "Train Epoch: 7 [1600/1780 (400%)]\tLoss: 1.481588\n",
      "\n",
      "Test set: Avg. loss: 7.1178, Accuracy: 186/445 (42%)\n",
      "\n",
      "Train Epoch: 8 [0/1780 (0%)]\tLoss: 1.312329\n",
      "Train Epoch: 8 [400/1780 (100%)]\tLoss: 1.387647\n",
      "Train Epoch: 8 [800/1780 (200%)]\tLoss: 1.408620\n",
      "Train Epoch: 8 [1200/1780 (300%)]\tLoss: 1.559429\n",
      "Train Epoch: 8 [1600/1780 (400%)]\tLoss: 1.476752\n",
      "\n",
      "Test set: Avg. loss: 7.0922, Accuracy: 184/445 (41%)\n",
      "\n",
      "Train Epoch: 9 [0/1780 (0%)]\tLoss: 1.305153\n",
      "Train Epoch: 9 [400/1780 (100%)]\tLoss: 1.380397\n",
      "Train Epoch: 9 [800/1780 (200%)]\tLoss: 1.402449\n",
      "Train Epoch: 9 [1200/1780 (300%)]\tLoss: 1.558586\n",
      "Train Epoch: 9 [1600/1780 (400%)]\tLoss: 1.471806\n",
      "\n",
      "Test set: Avg. loss: 7.0665, Accuracy: 188/445 (42%)\n",
      "\n",
      "Train Epoch: 10 [0/1780 (0%)]\tLoss: 1.297912\n",
      "Train Epoch: 10 [400/1780 (100%)]\tLoss: 1.373056\n",
      "Train Epoch: 10 [800/1780 (200%)]\tLoss: 1.396219\n",
      "Train Epoch: 10 [1200/1780 (300%)]\tLoss: 1.557748\n",
      "Train Epoch: 10 [1600/1780 (400%)]\tLoss: 1.466909\n",
      "\n",
      "Test set: Avg. loss: 7.0406, Accuracy: 189/445 (42%)\n",
      "\n",
      "Train Epoch: 11 [0/1780 (0%)]\tLoss: 1.290621\n",
      "Train Epoch: 11 [400/1780 (100%)]\tLoss: 1.365595\n",
      "Train Epoch: 11 [800/1780 (200%)]\tLoss: 1.389702\n",
      "Train Epoch: 11 [1200/1780 (300%)]\tLoss: 1.557140\n",
      "Train Epoch: 11 [1600/1780 (400%)]\tLoss: 1.461746\n",
      "\n",
      "Test set: Avg. loss: 7.0148, Accuracy: 188/445 (42%)\n",
      "\n",
      "Train Epoch: 12 [0/1780 (0%)]\tLoss: 1.283369\n",
      "Train Epoch: 12 [400/1780 (100%)]\tLoss: 1.358110\n",
      "Train Epoch: 12 [800/1780 (200%)]\tLoss: 1.383214\n",
      "Train Epoch: 12 [1200/1780 (300%)]\tLoss: 1.556258\n",
      "Train Epoch: 12 [1600/1780 (400%)]\tLoss: 1.456745\n",
      "\n",
      "Test set: Avg. loss: 6.9887, Accuracy: 190/445 (43%)\n",
      "\n",
      "Train Epoch: 13 [0/1780 (0%)]\tLoss: 1.275910\n",
      "Train Epoch: 13 [400/1780 (100%)]\tLoss: 1.350425\n",
      "Train Epoch: 13 [800/1780 (200%)]\tLoss: 1.376323\n",
      "Train Epoch: 13 [1200/1780 (300%)]\tLoss: 1.555536\n",
      "Train Epoch: 13 [1600/1780 (400%)]\tLoss: 1.451555\n",
      "\n",
      "Test set: Avg. loss: 6.9626, Accuracy: 189/445 (42%)\n",
      "\n",
      "Train Epoch: 14 [0/1780 (0%)]\tLoss: 1.268536\n",
      "Train Epoch: 14 [400/1780 (100%)]\tLoss: 1.342748\n",
      "Train Epoch: 14 [800/1780 (200%)]\tLoss: 1.369330\n",
      "Train Epoch: 14 [1200/1780 (300%)]\tLoss: 1.554791\n",
      "Train Epoch: 14 [1600/1780 (400%)]\tLoss: 1.446360\n",
      "\n",
      "Test set: Avg. loss: 6.9365, Accuracy: 190/445 (43%)\n",
      "\n",
      "Train Epoch: 15 [0/1780 (0%)]\tLoss: 1.261115\n",
      "Train Epoch: 15 [400/1780 (100%)]\tLoss: 1.334972\n",
      "Train Epoch: 15 [800/1780 (200%)]\tLoss: 1.362204\n",
      "Train Epoch: 15 [1200/1780 (300%)]\tLoss: 1.553994\n",
      "Train Epoch: 15 [1600/1780 (400%)]\tLoss: 1.441087\n",
      "\n",
      "Test set: Avg. loss: 6.9103, Accuracy: 190/445 (43%)\n",
      "\n",
      "Train Epoch: 16 [0/1780 (0%)]\tLoss: 1.253547\n",
      "Train Epoch: 16 [400/1780 (100%)]\tLoss: 1.327147\n",
      "Train Epoch: 16 [800/1780 (200%)]\tLoss: 1.354908\n",
      "Train Epoch: 16 [1200/1780 (300%)]\tLoss: 1.553290\n",
      "Train Epoch: 16 [1600/1780 (400%)]\tLoss: 1.435789\n",
      "\n",
      "Test set: Avg. loss: 6.8840, Accuracy: 190/445 (43%)\n",
      "\n",
      "Train Epoch: 17 [0/1780 (0%)]\tLoss: 1.246048\n",
      "Train Epoch: 17 [400/1780 (100%)]\tLoss: 1.319247\n",
      "Train Epoch: 17 [800/1780 (200%)]\tLoss: 1.347533\n",
      "Train Epoch: 17 [1200/1780 (300%)]\tLoss: 1.552520\n",
      "Train Epoch: 17 [1600/1780 (400%)]\tLoss: 1.430461\n",
      "\n",
      "Test set: Avg. loss: 6.8577, Accuracy: 193/445 (43%)\n",
      "\n",
      "Train Epoch: 18 [0/1780 (0%)]\tLoss: 1.238487\n",
      "Train Epoch: 18 [400/1780 (100%)]\tLoss: 1.311290\n",
      "Train Epoch: 18 [800/1780 (200%)]\tLoss: 1.339777\n",
      "Train Epoch: 18 [1200/1780 (300%)]\tLoss: 1.552265\n",
      "Train Epoch: 18 [1600/1780 (400%)]\tLoss: 1.424911\n",
      "\n",
      "Test set: Avg. loss: 6.8314, Accuracy: 193/445 (43%)\n",
      "\n",
      "Train Epoch: 19 [0/1780 (0%)]\tLoss: 1.230981\n",
      "Train Epoch: 19 [400/1780 (100%)]\tLoss: 1.303325\n",
      "Train Epoch: 19 [800/1780 (200%)]\tLoss: 1.331913\n",
      "Train Epoch: 19 [1200/1780 (300%)]\tLoss: 1.551490\n",
      "Train Epoch: 19 [1600/1780 (400%)]\tLoss: 1.419381\n",
      "\n",
      "Test set: Avg. loss: 6.8051, Accuracy: 193/445 (43%)\n",
      "\n",
      "Train Epoch: 20 [0/1780 (0%)]\tLoss: 1.223428\n",
      "Train Epoch: 20 [400/1780 (100%)]\tLoss: 1.295272\n",
      "Train Epoch: 20 [800/1780 (200%)]\tLoss: 1.323861\n",
      "Train Epoch: 20 [1200/1780 (300%)]\tLoss: 1.550774\n",
      "Train Epoch: 20 [1600/1780 (400%)]\tLoss: 1.413850\n",
      "\n",
      "Test set: Avg. loss: 6.7787, Accuracy: 194/445 (44%)\n",
      "\n",
      "Train Epoch: 21 [0/1780 (0%)]\tLoss: 1.215551\n",
      "Train Epoch: 21 [400/1780 (100%)]\tLoss: 1.287124\n",
      "Train Epoch: 21 [800/1780 (200%)]\tLoss: 1.315676\n",
      "Train Epoch: 21 [1200/1780 (300%)]\tLoss: 1.550074\n",
      "Train Epoch: 21 [1600/1780 (400%)]\tLoss: 1.408190\n",
      "\n",
      "Test set: Avg. loss: 6.7525, Accuracy: 195/445 (44%)\n",
      "\n",
      "Train Epoch: 22 [0/1780 (0%)]\tLoss: 1.207770\n",
      "Train Epoch: 22 [400/1780 (100%)]\tLoss: 1.279002\n",
      "Train Epoch: 22 [800/1780 (200%)]\tLoss: 1.307331\n",
      "Train Epoch: 22 [1200/1780 (300%)]\tLoss: 1.549221\n",
      "Train Epoch: 22 [1600/1780 (400%)]\tLoss: 1.402593\n",
      "\n",
      "Test set: Avg. loss: 6.7261, Accuracy: 196/445 (44%)\n",
      "\n",
      "Train Epoch: 23 [0/1780 (0%)]\tLoss: 1.199978\n",
      "Train Epoch: 23 [400/1780 (100%)]\tLoss: 1.270861\n",
      "Train Epoch: 23 [800/1780 (200%)]\tLoss: 1.298876\n",
      "Train Epoch: 23 [1200/1780 (300%)]\tLoss: 1.548558\n",
      "Train Epoch: 23 [1600/1780 (400%)]\tLoss: 1.396973\n",
      "\n",
      "Test set: Avg. loss: 6.6998, Accuracy: 195/445 (44%)\n",
      "\n",
      "Train Epoch: 24 [0/1780 (0%)]\tLoss: 1.192189\n",
      "Train Epoch: 24 [400/1780 (100%)]\tLoss: 1.262719\n",
      "Train Epoch: 24 [800/1780 (200%)]\tLoss: 1.290226\n",
      "Train Epoch: 24 [1200/1780 (300%)]\tLoss: 1.547698\n",
      "Train Epoch: 24 [1600/1780 (400%)]\tLoss: 1.391356\n",
      "\n",
      "Test set: Avg. loss: 6.6735, Accuracy: 196/445 (44%)\n",
      "\n",
      "Train Epoch: 25 [0/1780 (0%)]\tLoss: 1.184354\n",
      "Train Epoch: 25 [400/1780 (100%)]\tLoss: 1.254538\n",
      "Train Epoch: 25 [800/1780 (200%)]\tLoss: 1.281442\n",
      "Train Epoch: 25 [1200/1780 (300%)]\tLoss: 1.546891\n",
      "Train Epoch: 25 [1600/1780 (400%)]\tLoss: 1.385665\n",
      "\n",
      "Test set: Avg. loss: 6.6473, Accuracy: 197/445 (44%)\n",
      "\n",
      "Train Epoch: 26 [0/1780 (0%)]\tLoss: 1.176489\n",
      "Train Epoch: 26 [400/1780 (100%)]\tLoss: 1.246360\n",
      "Train Epoch: 26 [800/1780 (200%)]\tLoss: 1.272541\n",
      "Train Epoch: 26 [1200/1780 (300%)]\tLoss: 1.546026\n",
      "Train Epoch: 26 [1600/1780 (400%)]\tLoss: 1.380102\n",
      "\n",
      "Test set: Avg. loss: 6.6213, Accuracy: 198/445 (44%)\n",
      "\n",
      "Train Epoch: 27 [0/1780 (0%)]\tLoss: 1.168720\n",
      "Train Epoch: 27 [400/1780 (100%)]\tLoss: 1.238162\n",
      "Train Epoch: 27 [800/1780 (200%)]\tLoss: 1.263642\n",
      "Train Epoch: 27 [1200/1780 (300%)]\tLoss: 1.545256\n",
      "Train Epoch: 27 [1600/1780 (400%)]\tLoss: 1.374434\n",
      "\n",
      "Test set: Avg. loss: 6.5952, Accuracy: 200/445 (45%)\n",
      "\n",
      "Train Epoch: 28 [0/1780 (0%)]\tLoss: 1.160929\n",
      "Train Epoch: 28 [400/1780 (100%)]\tLoss: 1.230029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 28 [800/1780 (200%)]\tLoss: 1.254456\n",
      "Train Epoch: 28 [1200/1780 (300%)]\tLoss: 1.544701\n",
      "Train Epoch: 28 [1600/1780 (400%)]\tLoss: 1.368842\n",
      "\n",
      "Test set: Avg. loss: 6.5692, Accuracy: 203/445 (46%)\n",
      "\n",
      "Train Epoch: 29 [0/1780 (0%)]\tLoss: 1.153077\n",
      "Train Epoch: 29 [400/1780 (100%)]\tLoss: 1.221809\n",
      "Train Epoch: 29 [800/1780 (200%)]\tLoss: 1.245237\n",
      "Train Epoch: 29 [1200/1780 (300%)]\tLoss: 1.543923\n",
      "Train Epoch: 29 [1600/1780 (400%)]\tLoss: 1.363212\n",
      "\n",
      "Test set: Avg. loss: 6.5434, Accuracy: 203/445 (46%)\n",
      "\n",
      "Train Epoch: 30 [0/1780 (0%)]\tLoss: 1.145301\n",
      "Train Epoch: 30 [400/1780 (100%)]\tLoss: 1.213645\n",
      "Train Epoch: 30 [800/1780 (200%)]\tLoss: 1.236062\n",
      "Train Epoch: 30 [1200/1780 (300%)]\tLoss: 1.543014\n",
      "Train Epoch: 30 [1600/1780 (400%)]\tLoss: 1.357490\n",
      "\n",
      "Test set: Avg. loss: 6.5176, Accuracy: 207/445 (47%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 30\n",
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
